import torch as th
from copy import deepcopy

class FuzeParameter(th.nn.Parameter):
    '''
    TODO: Finish the class
    THIS CODE IS GENERATED BY DEEPSEEK, REALLY BAD
    '''
    def __new__(cls, pretrained_param, new_param, *args, **kwargs):
        print('__new__: ', args, kwargs)
        alpha = kwargs['alpha'] if 'alpha' in kwargs else args[0] 
        data = pretrained_param.data + alpha * new_param.data
        return super().__new__(cls, data, requires_grad=True)

    def __init__(self, pretrained_param: th.nn.Parameter, new_param: th.nn.Parameter, alpha=0.5):
        super(FuzeParameter, self).__init__()
        self.pretrained_param = pretrained_param.requires_grad_(False)
        self.new_param = new_param.requires_grad_(True)
        self.alpha = alpha
    
    @property
    def grad(self):
        # Gradient is stored in a
        print('Calling grad property', self.new_param.grad)
        return self.new_param.grad
    
    @grad.setter
    def grad(self, value):
        # Redirect gradient accumulation to a
        print('Calling grad setter', value)
        if self.new_param.grad is None:
            self.new_param.grad = value
        else:
            self.new_param.grad += value
    
    @property
    def data(self):
        # Always return the current sum a + b
        return self.new_param.data + self.pretrained_param.data
    
    @data.setter
    def data(self, value):
        # Prevent direct modification to maintain the invariant: value = a + b
        raise RuntimeError("Direct assignment to MyParameter.data is not allowed. Modify a or b instead.")


class FuzeLayer(th.nn.Module):
    def __init__(self, pretrained_layer: th.nn.Module, new_layer: th.nn.Module, alpha=0.5):
        super(FuzeLayer, self).__init__()
        self.pretrained_layer = pretrained_layer
        self.new_layer = new_layer
        self.alpha = alpha
        # self.alpha = th.nn.Parameter(th.Tensor([alpha]), requires_grad=True)
        self._sanity_check()
        self.pretrained_layer.requires_grad_(False)
    
    def _sanity_check(self):
        if self.pretrained_layer.__class__ != self.new_layer.__class__:
            raise ValueError(f"The two layers must be of the same type, \
                             get {self.pretrained_layer.__class__} and {self.new_layer.__class__}.")
        # if self.pretrained_layer.device != self.new_layer.device:
        #     raise ValueError("The two layers must be on the same device.")
        pretrained_params = {n: p.shape for n, p in self.pretrained_layer.named_parameters()}
        new_params = {n: p.shape for n, p in self.new_layer.named_parameters()}
        if pretrained_params.keys() != new_params.keys():
            raise ValueError("The two layers must have the same parameters.")
        for p1, p2 in zip(pretrained_params.values(), new_params.values()):
            if p1 != p2:
                raise ValueError(f"The two layers must have the same parameter shapes, get {p1} and {p2}.")


    def forward(self, *args, **kwargs):
        '''
        Premitive version: simply add the outputs of two layers.
        TODO: implement a more advanced fusion strategy.
        '''
        pretrained_output = self.pretrained_layer(*args, **kwargs)
        new_output = self.new_layer(*args, **kwargs)
        return pretrained_output + self.alpha * new_output

if __name__ == "__main__":
    a = th.nn.Parameter(th.randn(3, 2))
    b = th.nn.Parameter(th.randn(3, 2))
    a_ = deepcopy(a)
    b_ = deepcopy(b)
    c=a+0.2*b
    print(a)
    print(b)
    print(c)
    fuze_param = FuzeParameter(a, b, alpha=0.2)
    print(fuze_param)
    print((c==fuze_param).all())
    print(a*c+b)
    print(a*fuze_param+b)
    print((a*fuze_param+b==a*c+b).all())
    print('==================')
    opt=th.optim.SGD([fuze_param], lr=0.01)
    y=fuze_param.sum()
    y.backward()
    print(fuze_param.grad)
    opt.step()
    print(fuze_param)
    print(a)
    print(b)
